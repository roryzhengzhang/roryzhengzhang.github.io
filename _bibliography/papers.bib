---
---

@inproceedings{EMNLP2023,
      title={Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations}, 
      author={Yuan Tian and Zheng Zhang and Zheng Ning and Toby Jia-Jun Li and Jonathan K. Kummerfeld and Tianyi Zhang},
      year={2023},
      abstract = "Relational databases play an important role in this Big Data era. However, it is challenging for non-experts to fully unleash the analytical power of relational databases, since they are not familiar with database languages such as SQL. Many techniques have been proposed to automatically generate SQL from natural language, but they suffer from two issues: (1) they still make many mistakes, particularly for complex queries, and (2) they do not provide a flexible way for non-expert users to validate and refine the incorrect queries. To address these issues, we introduce a new interaction mechanism that allows users directly edit a step-by-step explanation of an incorrect SQL to fix SQL errors. Experiments on the Spider benchmark show that our approach outperforms three SOTA approaches by at least 31.6% in terms of execution accuracy. A user study with 24 participants further shows that our approach helped users solve significantly more SQL tasks with less time and higher confidence, demonstrating its potential to expand access to databases, particularly for non-experts.",
      booktitle = "The 2023 Conference on Empirical Methods in Natural Language Processing",
      preview = {EMNLP2023-teaser.png},
      month = december,
      abbr = {EMNLP 2023},
      pdf={EMNLP-2023.pdf},
      selected = {true}
}

@inproceedings{PEANUT2023,
      title={PEANUT: A Human-AI Collaborative Tool for Annotating Audio-Visual Data}, 
      author={Zheng Zhang and Zheng Ning and Chenliang Xu and Yapeng Tian and Toby Jia-Jun Li},
      year={2023},
      abstract = "Audio-visual learning seeks to enhance the computer's multi-modal perception leveraging the correlation between the auditory and visual modalities. Despite their many useful downstream tasks, such as video retrieval, AR/VR, and accessibility, the performance and adoption of existing audio-visual models have been impeded by the availability of high-quality datasets. Annotating audio-visual datasets is laborious, expensive, and time-consuming. To address this challenge, we designed and developed an efficient audio-visual annotation tool called Peanut. Peanut's human-AI collaborative pipeline separates the multi-modal task into two single-modal tasks, and utilizes state-of-the-art object detection and sound-tagging models to reduce the annotators' effort to process each frame and the number of manually-annotated frames needed. A within-subject user study with 20 participants found that Peanut can significantly accelerate the audio-visual data annotation process while maintaining high annotation accuracy.",
      booktitle = "Proceedings of the 2023 ACM Symposium on User Interface Software and Technology",
      preview = {PEANUT2023-teaser.png},
      month = october,
      abbr = {UIST 2023},
      pdf={PEANUT-2023.pdf},
      selected = {true}
}

@inproceedings{VISAR2023,
      title={VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping}, 
      author={Zheng Zhang and Jie Gao and Ranjodh Singh Dhaliwal and Toby Jia-Jun Li},
      year={2023},
      month = october,
      abstract = "In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confirmed the usability and effectiveness of VISAR in facilitating the argumentative writing planning process.",
      abbr = {UIST 2023},
      booktitle = "Proceedings of the 2023 ACM Symposium on User Interface Software and Technology",
      preview = {visar2023-teaser.png},
      pdf={VISAR-2023.pdf},
      selected = {true}
}

@inproceedings{PATAT2023,
  title = "PaTAT: Human-AI Collaborative Qualitative Coding with Explainable Interactive Rule Synthesis",
  author = "Gebreegziabher*, Simret Araya and Zhang*, Zheng and Tang, Xiaohang and Meng, Yihao and Glassman, Elena and Li, Toby Jia-jun",
  booktitle = "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
  year = "2023",
  month = april,
  abstract = "The use of AI assistance in data annotation has made significant progress. However, qualitative coding in thematic analysis, as a specific
  type of annotation task, has unique characteristics that make effective human-AI collaboration difficult. Informed by a formative
  study, we designed PaTAT, a new AI-enabled tool that uses an interactive program synthesis approach to learn flexible and expressive
  patterns of user-annotated codes in real-time as users annotate the data. To accommodate the ambiguous, uncertain, and iterative
  nature of thematic analysis, the use of user-interpretable patterns allows users to understand and validate what the system has learned,
  make direct fixes, and easily revise, split, or merge previously annotated codes. This new approach also helps human users to learn data
  characteristics and form new theories in addition to facilitating the “learning” of the AI model. PaTAT’s usefulness and effectiveness
  were evaluated in a lab user study.",
  abbr = {CHI 2023},
  preview={PaTAT2023-teaser.png},
  pdf={PATAT-CHI-2023.pdf},
  selected = {true}
}

@inproceedings{NL2SQL2023,
  title = "An Empirical Study of Model Errors and User Error Discovery and Repair Strategies in Natural Language Database Queries",
  author = "Ning*, Zheng and Zhang*, Zheng and Sun, Tianyi and Tian, Yuan and Zhang, Tianyi and Li, Toby Jia-jun",
  booktitle = "Proceedings of the 28th International Conference on Intelligent User Interfaces",
  year = "2023",
  month = march,
  abstract = "Recent advances in machine learning (ML) and natural language processing (NLP) have led to significant improvement in natural
language interfaces for structured databases (NL2SQL). Despite the great strides, the overall accuracy of NL2SQL models is still far from
being perfect (∼75% on the Spider benchmark). In practice, this requires users to discern incorrect SQL queries generated by a model
and manually fix them when using NL2SQL models. Currently, there is a lack of comprehensive understanding about the common
errors in auto-generated SQLs and the effective strategies to recognize and fix such errors. To bridge the gap, we (1) performed an
in-depth analysis of errors made by three state-of-the-art NL2SQL models; (2) distilled a taxonomy of NL2SQL model errors; and
(3) conducted a within-subjects user study with 26 participants to investigate the effectiveness of three representative interactive
mechanisms for error discovery and repair in NL2SQL. Findings from this paper shed light on the design of future error discovery and
repair strategies for natural language interfaces for structured databases.",
  abbr = {IUI 2023},
  preview={NL2SQL2023-teaser.png},
  pdf={NL2SQL-IUI-2023.pdf},
  selected = {true}
}

@inproceedings{fairytaleQA2022,
    title = "Fantastic Questions and Where to Find Them: {F}airytale{QA} {--} An Authentic Dataset for Narrative Comprehension",
    author = "Xu, Ying  and
      Wang, Dakuo  and
      Yu, Mo  and
      Ritchie, Daniel  and
      Yao, Bingsheng  and
      Wu, Tongshuang  and
      Zhang, Zheng  and
      Li, Toby  and
      Bradford, Nora  and
      Sun, Branda  and
      Hoang, Tran  and
      Sang, Yisi  and
      Hou, Yufang  and
      Ma, Xiaojuan  and
      Yang, Diyi  and
      Peng, Nanyun  and
      Yu, Zhou  and
      Warschauer, Mark",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.34",
    doi = "10.18653/v1/2022.acl-long.34",
    pages = "447--460",
    abstract = "Question answering (QA) is a fundamental means to facilitate assessment and training of narrative comprehension skills for both machines and young children, yet there is scarcity of high-quality QA datasets carefully designed to serve this purpose. In particular, existing datasets rarely distinguish fine-grained reading skills, such as the understanding of varying narrative elements. Drawing on the reading education research, we introduce FairytaleQA, a dataset focusing on narrative comprehension of kindergarten to eighth-grade students. Generated by educational experts based on an evidence-based theoretical framework, FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly stories, covering seven types of narrative elements or relations. Our dataset is valuable in two folds: First, we ran existing QA models on our dataset and confirmed that this annotation helps assess models{'} fine-grained learning skills. Second, the dataset supports question generation (QG) task in the education domain. Through benchmarking with QG models, we show that the QG model trained on FairytaleQA is capable of asking high-quality and more diverse questions.",
    abbr={ACL 2022},
    pdf={fairytaleQA2022.pdf},
    preview={fairytaleQA2022-teaser.png},
    selected={true}
}

@inproceedings{storybookQAgeneration2022,
    title = "It is {AI}{'}s Turn to Ask Humans a Question: Question-Answer Pair Generation for Children{'}s Story Books",
    author = "Yao, Bingsheng  and
      Wang, Dakuo  and
      Wu, Tongshuang  and
      Zhang, Zheng  and
      Li, Toby  and
      Yu, Mo  and
      Xu, Ying",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.54",
    doi = "10.18653/v1/2022.acl-long.54",
    pages = "731--744",
    pdf={storybookQA2022.pdf},
    abstract = "Existing question answering (QA) techniques are created mainly to answer questions asked by humans. But in educational applications, teachers often need to decide what questions they should ask, in order to help students to improve their narrative understanding capabilities. We design an automated question-answer generation (QAG) system for this education scenario: given a story book at the kindergarten to eighth-grade level as input, our system can automatically generate QA pairs that are capable of testing a variety of dimensions of a student{'}s comprehension skills. Our proposed QAG model architecture is demonstrated using a new expert-annotated FairytaleQA dataset, which has 278 child-friendly storybooks with 10,580 QA pairs. Automatic and human evaluations show that our model outperforms state-of-the-art QAG baseline systems. On top of our QAG system, we also start to build an interactive story-telling application for the future real-world deployment in this educational scenario.",
    abbr={ACL 2022},
    preview={storybookQA2022-teaser.png},
    selected={true}
}

@inproceedings{storybuddy2022,
  author = {Zhang, Zheng and Xu, Ying and Wang, Yanhao and Yao, Bingsheng and Ritchie, Daniel and Wu, Tongshuang and Yu, Mo and Wang, Dakuo and Li, Toby Jia-Jun},
  title = {StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement},
  year = {2022},
  isbn = {9781450391573},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3491102.3517479},
  doi = {10.1145/3491102.3517479},
  abstract = {Despite its benefits for children’s skill development and parent-child bonding, many parents do not often engage in interactive storytelling by having story-related dialogues with their child due to limited availability or challenges in coming up with appropriate questions. While recent advances made AI generation of questions from stories possible, the fully-automated approach excludes parent involvement, disregards educational goals, and underoptimizes for child engagement. Informed by need-finding interviews and participatory design (PD) results, we developed StoryBuddy, an AI-enabled system for parents to create interactive storytelling experiences. StoryBuddy’s design highlighted the need for accommodating dynamic user needs between the desire for parent involvement and parent-child bonding and the goal of minimizing parent intervention when busy. The PD revealed varied assessment and educational goals of parents, which StoryBuddy addressed by supporting configuring question types and tracking child progress. A user study validated StoryBuddy’s usability and suggested design insights for future parent-AI collaboration systems.},
  booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  articleno = {218},
  numpages = {21},
  pdf={storybuddy2022.pdf},
  keywords = {interactive storytelling, voice user interfaces, dialogic reading, human-AI collaboration, co-reading, child-agent interactions},
  location = {New Orleans, LA, USA},
  series = {CHI '22},
  abbr={CHI 2022},
  preview={storybuddy2022-teaser.png},
  selected={true}
}

@inproceedings{racegenderbeauty2020,
  author = {Leung*, Weiwen and Zhang*, Zheng and Jibuti, Daviti and Zhao, Jinhao and Klein, Maximilian and Pierce, Casey and Robert, Lionel and Zhu, Haiyi},
  title = {Race, Gender and Beauty: The Effect of Information Provision on Online Hiring Biases},
  year = {2020},
  isbn = {9781450367080},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3313831.3376874},
  doi = {10.1145/3313831.3376874},
  pdf={racegenderbeauty2020.pdf},
  abstract = {We conduct a study of hiring bias on a simulation platform where we ask Amazon MTurk participants to make hiring decisions for a mathematically intensive task. Our findings suggest hiring biases against Black workers and less attractive workers, and preferences towards Asian workers, female workers and more attractive workers. We also show that certain UI designs, including provision of candidates' information at the individual level and reducing the number of choices, can significantly reduce discrimination. However, provision of candidate's information at the subgroup level can increase discrimination. The results have practical implications for designing better online freelance marketplaces.},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages = {1–11},
  numpages = {11},
  keywords = {gig economy, discrimination, hiring},
  location = {Honolulu, HI, USA},
  series = {CHI '20},
  abbr={CHI 2020},
  preview={racegenderbeauty2020-teaser.png},
  selected={true}
}

@inproceedings{eplainthroughui2019,
  author = {Cheng, Hao-Fei and Wang, Ruotong and Zhang, Zheng and O'Connell, Fiona and Gray, Terrance and Harper, F. Maxwell and Zhu, Haiyi},
  title = {Explaining Decision-Making Algorithms through UI: Strategies to Help Non-Expert Stakeholders},
  year = {2019},
  isbn = {9781450359702},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3290605.3300789},
  doi = {10.1145/3290605.3300789},
  pdf={explainthroughui2019.pdf},
  abstract = {Increasingly, algorithms are used to make important decisions across society. However, these algorithms are usually poorly understood, which can reduce transparency and evoke negative emotions. In this research, we seek to learn design principles for explanation interfaces that communicate how decision-making algorithms work, in order to help organizations explain their decisions to stakeholders, or to support users' "right to explanation". We conducted an online experiment where 199 participants used different explanation interfaces to understand an algorithm for making university admissions decisions. We measured users' objective and self-reported understanding of the algorithm. Our results show that both interactive explanations and "white-box" explanations (i.e. that show the inner workings of an algorithm) can improve users' comprehension. Although the interactive approach is more effective at improving comprehension, it comes with a trade-off of taking more time. Surprisingly, we also find that users' trust in algorithmic decisions is not affected by the explanation interface or their level of comprehension of the algorithm.},
  booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages = {1–12},
  numpages = {12},
  keywords = {explanation interfaces, algorithmic decision-making},
  location = {Glasgow, Scotland Uk},
  series = {CHI '19},
  abbr={CHI 2019},
  preview={explainthroughui2019-teaser.png},
  selected={true}
}